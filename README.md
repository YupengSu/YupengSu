## Hi there 👋

I am a senior student from <a href='https://osa.sustech.edu.cn/index.php?g=School&m=College&a=syjj&term=67&pterm=66'> ZhiCheng College (致诚书院)</a> and <a href='https://haoyulab.sme.sustech.edu.cn/'> School of Microelectronics (深港微电子学院)</a> in Southern University of Science and Technology, advice by <a href='https://scholar.google.com/citations?user=X06MaeYAAAAJ&hl=en'> Hao Yu (余浩)</a>. I also work at <a href='https://hku-ngai.github.io/'> Hong Kong University</a> as a Student RA and collaborate with <a href='https://scholar.google.com/citations?user=PM_uMYIAAAAJ&hl=en'> Ngai Wong </a> closely.

My research interests includes Efficient/Low-resource methods for NLP, Model Edge Deployment and AI accelerator. I have published and submitted papers with total <a href='https://scholar.google.com/citations?user=zZ8lS-UAAAAJ'><img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FYupengSu%2FYupengSu.github.io%40google-scholar-stats%2Fgs_data_shieldsio.json&amp;logo=Google%20Scholar&amp;labelColor=f6f6f6&amp;color=9cf&amp;style=flat&amp;label=citations"></a> at the top international AI conferences such as DAC, NAACL.

### 📎 Homepages

- Personal Pages: [https://yupengsu.github.io](https://yupengsu.github.io) (updated recently🔥)
- Linkedin: [https://www.linkedin.com/in/yupeng-su](https://www.linkedin.com/in/yupeng-su)
- Google Scholar: [https://scholar.google.com/citations?user=4FA6C0AAAAAJ](https://scholar.google.com/citations?user=zZ8lS-UAAAAJ)
- DBLP: [https://dblp.org/pid/75/6568-6.html](https://dblp.org/pid/224/1935.html)

### 🔥 News

- *2024.09* : &nbsp; I join HKU Ngai Wong's Lab as Student RA and collaborate with PloyU Hongxia Yang's Lab!
- *2024.02* : &nbsp; 🎉 Our paper are accepted by DAC 2024.

### 📝 Publications 

`ARXIV` [LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models](https://arxiv.org/pdf/2408.10631), **Yupeng Su**, Ziyi Guan, Xiaoqun Liu, Tianlai Jin, Dongkuan Wu, Graziano Chesi, Ngai Wong, Hao Yu.

`DAC 2024` [APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models](https://arxiv.org/pdf/2402.14866), Ziyi Guan, Hantao Huang, **Yupeng Su**, Hong Huang, Ngai Wong, Hao Yu.
