## Hi there üëã

I am a Ph.D. student in Computer Science at the University of California, Santa Barbara (UCSB), advised by Prof. Zheng Zhang. I previously received my B.E. degree in Microelectronics from the Southern University of Science and Technology (SUSTech), advised by Prof. Hao Yu.

My research focuses on efficient pre-training and post-training optimization for large language models (LLMs), particularly quantization, sparsification, and low-rank decomposition for improving training and inference efficiency. I am also interested in high-performance computing and AI accelerators, including compiler optimization and GPU/FPGA kernel adaptation for efficient and energy-aware deployment. I have published and submitted papers with total <a href='https://scholar.google.com/citations?user=zZ8lS-UAAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FYupengSu%2FYupengSu.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a> at the top international conferences and journals.

If you are interested in my research or have any questions, also please feel free to contact me at <a href='mailto:yupengsu06@gmail.com'> yupengsu06@gmail.com</a> or <a href='mailto:yupengsu@ucsb.edu.'> yupengsu@ucsb.edu</a>.

### üìé Homepages

- Personal Pages: [https://yupengsu.github.io](https://yupengsu.github.io) (updated recentlyüî•)
- Linkedin: [https://www.linkedin.com/in/yupeng-su](https://www.linkedin.com/in/yupeng-su)
- Google Scholar: [https://scholar.google.com/citations?user=4FA6C0AAAAAJ](https://scholar.google.com/citations?user=zZ8lS-UAAAAJ)
- DBLP: [https://dblp.org/pid/224/1935.html](https://dblp.org/pid/224/1935.html)

### üî• News

- *2025.07* : &nbsp;  üéâ Our paper LLM-Barber is accepted by IEEE/ACM ICCAD 2025.
- *2025.05* : &nbsp;  üéâ Our paper EdgeLLM is accepted by IEEE TCAS I: Regular Papers.
- *2025.02* : &nbsp; I have been admitted to the PhD program at University of California, Santa Barbara (UCSB)!
- *2024.09* : &nbsp; I join HKU Next Gen AI(NGai) Lab as Student RA and collaborate with PloyU Hongxia Yang's Lab!
- *2024.02* : &nbsp; üéâ Our paper APTQ is accepted by IEEE/ACM DAC 2024.

### üìù Publications 

- `ICCAD 2025` [LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models](https://arxiv.org/abs/2408.10631), **Yupeng Su**, Ziyi Guan, Xiaoqun Liu, Tianlai Jin, Dongkuan Wu, Graziano Chesi, Ngai Wong, Hao Yu.

- `DAC 2024` [APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2402.14866), Ziyi Guan, Hantao Huang, **Yupeng Su**, Hong Huang, Ngai Wong, Hao Yu.

- `TCAS I` [EdgeLLM: A Highly Efficient CPU-FPGA Heterogeneous Edge Accelerator for Large Language Models](https://arxiv.org/abs/2407.21325), Mingqiang Huang, Ao Shen, Kai Li, Haoxiang Peng, Boyu Li, **Yupeng Su**, Hao Yu.

- `ARXIV` [Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning](https://arxiv.org/abs/2501.03035), Zhen Li, **Yupeng Su**, Runming Yang, Congkai Xie, Zheng Wang, Zhongwei Xie, Ngai Wong, Hongxia Yang.

- `ARXIV` [PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models](https://arxiv.org/abs/2509.16989), He Xiao, Runming Yang, Qingyao Yang, Wendong Xu, Zhen Li, **Yupeng Su**, Zhengwu Liu, Hongxia Yang, Ngai Wong.
